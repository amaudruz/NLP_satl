{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import *\n",
    "from fastai import *\n",
    "import torch\n",
    "from fastai.text import *\n",
    "from torchtext.datasets import Multi30k, LanguageModelingDataset\n",
    "from models import *\n",
    "from torchtext.data import *\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import os\n",
    "import csv\n",
    "from itertools import chain\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD LSTM Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the full implementation of a language model neural network using a pretrained AWD_LSTM architecture and finetuning it on a given as done for the [ULMFIT](https://arxiv.org/pdf/1801.06146.pdf) paper. Most of the upcoming code is heavily based on the [fastai](https://docs.fast.ai) library and its deep learning course, which has already a full implementation of the ulmfit approach for NLP. However considering the complexity of the fastai code and its simplicity to use we figured it would helpful for readers to get a full bottom up implementation using pytorch as a baseline. Still we expect that if you read this notebook, you have a good knowledge and understanding of RNNs, language modeling (see paper) and pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(x_1, x_2, x_3, ..., x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea of a language model is to learn how to \"speak\" the language which means that given a sentence $x = (x_1, x_2, x_3, ..., x_t)$ where a $x_i$ is the i-th word of the sentence, it needs to predict the vector $y = (y_1, y_2, y_3, ...,y_t)$ where $y_t = x_{t+1}$. In other words it needs to predict at each time step $t$ the next word $x_{t+1}$.\n",
    "\n",
    "How do we come up with sentences ? The ideal idea would be to have a giant text and split it into $t$ size sentences. Now in our situation, we only have tweets, so what we need to do is simply concatenate them using a special made worde in between them so our language model will be able to learn the difference.\n",
    "\n",
    "Now obviously, a neural network can't take strings as input, so we will have to numericalize them which means assigning a number to each of the words in the text. Now we need to be careful our entire set of tweets contains probably way too much different words. So what we do is we keep the words that are the most used in the text and assign those that are not used that much to a special word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataSet(data.Dataset):\n",
    "    \"\"\"Defines a dataset for language modeling.\"\"\"\n",
    "\n",
    "    def __init__(self, path, text_field, newline_eos=True,\n",
    "                 encoding='utf-8', **kwargs):\n",
    "        \"\"\"Create a LanguageModelingDataset given a path and a field.\n",
    "\n",
    "        Arguments:\n",
    "            path: Path to the data file.\n",
    "            text_field: The field that will be used for text data.\n",
    "            newline_eos: Whether to add an <eos> token for every newline in the\n",
    "                data file. Default: True.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        fields = [('text', text_field)]\n",
    "        text = []\n",
    "        with open(path, encoding=encoding) as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            count=0\n",
    "            for row in reader:\n",
    "                count+=1\n",
    "                if count ==1 :\n",
    "                    continue \n",
    "                text += text_field.preprocess(row[1])\n",
    "                if newline_eos:\n",
    "                    text.append(u'<eos>')\n",
    "                \n",
    "\n",
    "        examples = [data.Example.fromlist([text], fields)]\n",
    "        super(LMDataSet, self).__init__(\n",
    "            examples, fields, **kwargs)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_sample(self, ratio=0.2) :\n",
    "        text = self.examples[0].text\n",
    "        fs =len(text)\n",
    "        text_sample = text[:int(fs*ratio)]\n",
    "        ex_sample = Example.fromlist([text_sample], list(self.fields.items()))\n",
    "        return Dataset([ex_sample], list(self.fields.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(lm_dataset, ratio=0.8) :\n",
    "        text = lm_dataset.examples[0].text\n",
    "        fs =len(text)\n",
    "        \n",
    "        train = text[:int(fs*ratio)]\n",
    "        valid = text[int(fs*ratio):]\n",
    "        ex_train = Example.fromlist([train], list(lm_dataset.fields.items()))\n",
    "        ex_valid = Example.fromlist([valid], list(lm_dataset.fields.items()))\n",
    "        return Dataset([ex_train], list(lm_dataset.fields.items())), Dataset([ex_valid], list(lm_dataset.fields.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ds = LMDataSet('data/train_full.tsv', Field())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ds.fields['text'].build_vocab([lm_ds[0].text], max_size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ds_sample = lm_ds.get_sample(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = split_train_valid(lm_ds_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(347938, 86985, 43492376)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0].text), len(valid[0].text), len(lm_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "bptt = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabbb = train.fields['text'].vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it = BPTTIterator(train, bs, bptt)\n",
    "valid_it = BPTTIterator(valid, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWD_LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of the AWD LSTM architecture is of course the LSTM neural net. It is an improvement to the standart RNN way of dealing with sequential data (such as text). LSTM deals with the [vanishing/exploding gradient problem](https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb) that come up in simple RNNs using cell connection gates. For further intuition on 'why' most the upcoming implentations I recommend colah's blog post : [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM cell and equations](images/lstm.jpg)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM archutecture is composed of a repeated cell which is shown in the image above. Its inputs are :\n",
    "- **xt** which in our case is the embedding vector of the nth word of a batch of sentences\n",
    "- **ht-1** the output of the last cell just like in RNNs.\n",
    "- **ct-1** again output form last cell which is called the *cell state* used to prevent long-term dependencies problem.\n",
    "\n",
    "The $\\sigma$ reprenstents the [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function applied element-wise to its input. Both x and + connections are elemnt-wise multiplication and addition respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement it using the pytorch nn.Module class. We use a two big matrix multiplication to compute x*U and and h*U instead of 4 for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, x_s, h_s):\n",
    "        super().__init__()\n",
    "        self.h_s = h_s\n",
    "        self.x_s = x_s\n",
    "        self.U = nn.Linear(x_s,4*h_s)\n",
    "        self.W = nn.Linear(h_s,4*h_s)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        #inputs from last cell\n",
    "        h,c = state\n",
    "        \n",
    "        #computing itermedtiate gates\n",
    "        gates = (self.U(input) + self.W(h)).chunk(4, 1)\n",
    "        i_t,f_t,o_t = map(torch.sigmoid, gates[:3])\n",
    "        c_t = gates[3].tanh()\n",
    "        c = (f_t*c) + (i_t*c_t)\n",
    "        h = o_t * c.tanh()\n",
    "        \n",
    "        #outputting the usualt h output and the state to give to next cell if needed\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next building block of the LSTM is the LSTM layer wich consisit of appling the LSTM cell to each sequential input in a recurrent manner with each time forwarding its state to the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM layer](images/LSTM3.png)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, x_s, h_s):\n",
    "        super().__init__()\n",
    "        self.lstm_cell = LSTMCell(x_s, h_s)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        # divide the input in the sequence dimension to get x_0, x_1, x_2, ...\n",
    "        inputs = input.unbind(1)\n",
    "        \n",
    "        #prepare to store the output of each cell\n",
    "        outputs = []\n",
    "        \n",
    "        #applying the cell recursively \n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.lstm_cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        \n",
    "        #return the stacked outputs\n",
    "        return torch.stack(outputs, dim=1), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the state for the first cell we simply use tensors with only zeroes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before having fully implemented pytorch's LSTM module is stacking multiple LSTM layers one above each other as shown in the following diagram :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacked RNN](images/RNN_Stacking.png)\n",
    "(picture from : https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/recurrent_neural_networks.html?q= )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='pink'>pink rectangles</font>  : The sequential inputs  \n",
    "- <font color='green'>green rectangles</font>  : An LSTM cell, each line is a layer so the cells on the same line are the same\n",
    "- <font color='blue'>blue rectangles</font> : The sequential outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous implementations, we created the initial state **h0** and **c0** outside of the model at the same time as we gave the input. This means that we could create h0 and c0 with the right sizes by simply comparing it to the input sizes. This time around, we will create the initial state to give to all the layers inside the model itself. As the dimensions of the state depend on the batch size of the input given, we need to create the initial states at the start of the forward pass when we are given the input (and thus we know the batch size) with the help of the **reset()** method. We also want to keep the last states from last batch if the batch size did not change.\n",
    "\n",
    "We aso have to be careful of the sives of the hidden layers inputs. For example, on the image above, the first layer takes as input the initial sequence and outputs a hidden sequence whereas the second and third layers take as input a hidden sequence and outputs a hidden sequence. Because of that we must have a different sizes for the first layer and the other layers. And of course the same goes for the initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullLSTM(nn.Module):\n",
    "    def __init__(self, x_s, h_s, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm_layers = nn.ModuleList([LSTMLayer(x_s if i==0 else h_s, h_s ) for i in range(n_layers)])\n",
    "        self.bs = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        #get the batch size from the first dimension of the input \n",
    "        bs, sl, _ = input.size()\n",
    "        if self.bs != bs :\n",
    "            self.bs = bs\n",
    "            self.reset()\n",
    "       \n",
    "        # now we have the initial states and we can go through all the layers recursively\n",
    "        for j in range(self.n_layers) :\n",
    "            layer = self.lstm_layers[j]\n",
    "            input, self.hidden[j] = layer(input, self.hidden[j])\n",
    "                \n",
    "        \n",
    "        #return the outputs\n",
    "        return input\n",
    "    \n",
    "    def reset(self) :\n",
    "        st = next(self.parameters()).new(self.bs, h_s).zero_()\n",
    "        self.hidden = [(st, st) for l in range(self.n_layers)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is pretty much the same as pytorch's nn.LSTM. The difference is that pytorch uses CuDNN to make the computations faster. We will now use pytorch's implementation instead of ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization : Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual regularization techniques used in feed-forward and convolutional neural nets such as dropout and batchnorm do not work well in RNNs. The AWD LSTM uses extensions of those to regularize its model. Correspondigns ections of the [paper](https://arxiv.org/pdf/1708.02182.pdf) will be provided for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational dropout \n",
    "Section 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in variational dropout is to use the **same** drop out mask to a squential input over the sequence dimension. In essence, if you have an input x with shape *(bs, seq_len, x_s)*, the dropout mask will be of shape *(bs, 1, x_s)* and will be applied to each slice of sequence.\n",
    "This dropout will be used on each output/input of the LSTM layers. Additionally we divide every activations that have not been set to 0 by the mask by 1-p (p: probability of dropout) to keep the average. We use [broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html) to be efficient in the element-wise computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDropout(nn.Module) :\n",
    "    def __init__(self, p=0.5) :\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x) :\n",
    "        #The dropout should only be used during training and not eval \n",
    "        if not self.training or self.p == 0.: return x\n",
    "        #the mask\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        #element-wise multiplication with broadcasting\n",
    "        return x*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0916,  0.1921, -0.8202, -2.4365, -0.6699,  0.9976, -0.7295],\n",
       "          [-0.2314, -1.7133,  0.3183, -0.2061,  0.3275,  0.0333,  1.0133],\n",
       "          [ 0.3429,  1.8585,  1.9196,  0.4617, -0.1172, -0.8209,  0.4200]],\n",
       " \n",
       "         [[ 0.4948,  0.9259, -0.9224,  0.8257,  0.2613, -0.5878,  0.1486],\n",
       "          [-0.0370, -0.0519, -0.3507, -0.8274,  0.3976, -0.4022, -0.5860],\n",
       "          [-1.4251,  0.9739,  0.6816,  1.1509, -0.0826,  1.6007,  0.1701]],\n",
       " \n",
       "         [[ 0.0802, -0.6836, -2.3615, -0.4559, -1.7405,  0.1039,  0.7938],\n",
       "          [ 1.4682,  0.2345, -0.5690,  1.2756,  0.5497, -0.5573,  0.9412],\n",
       "          [ 1.6329,  0.0277, -0.8515, -2.3154,  1.4387,  0.5795, -0.7031]]]),\n",
       " tensor([[[-0.1309,  0.0000, -0.0000, -0.0000, -0.9570,  1.4251, -1.0421],\n",
       "          [-0.3306, -0.0000,  0.0000, -0.0000,  0.4679,  0.0476,  1.4475],\n",
       "          [ 0.4898,  0.0000,  0.0000,  0.0000, -0.1674, -1.1727,  0.6000]],\n",
       " \n",
       "         [[ 0.0000,  1.3228, -1.3177,  1.1796,  0.0000, -0.0000,  0.2123],\n",
       "          [-0.0000, -0.0741, -0.5010, -1.1820,  0.0000, -0.0000, -0.8372],\n",
       "          [-0.0000,  1.3913,  0.9737,  1.6442, -0.0000,  0.0000,  0.2430]],\n",
       " \n",
       "         [[ 0.1146, -0.9766, -3.3736, -0.6513, -2.4865,  0.0000,  0.0000],\n",
       "          [ 2.0974,  0.3350, -0.8129,  1.8223,  0.7852, -0.0000,  0.0000],\n",
       "          [ 2.3328,  0.0396, -1.2164, -3.3077,  2.0552,  0.0000, -0.0000]]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = VDropout(0.3)\n",
    "tst_input = torch.randn(3,3,7)\n",
    "tst_input, m(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the dropped is consistent in the second dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding dropout \n",
    "Section 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embedding dropout we simply nulifiy entire rows of the word embedding matrix with probability p. Again broadcastiong is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mEmbeddingDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [-1.7760, -0.3174,  1.9763,  0.8991, -1.2225, -1.6584,  0.3279],\n",
       "        [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = mEmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0,100,(8,))\n",
    "enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that entire rows have been dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight-dropout\n",
    "Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight dropout is a dropout applied to the weights inside the LSTM cells : U and W.\n",
    "\n",
    "In order to keep the speed of the LSTM layer, we simply replace the weight matrix of the LSTM by a masked version and keep the non-masked version. We can then simply apply the LSTM layer and it will use its new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the parameter in the nn.LSTM module containing the weights \n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class mWeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            #\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6194, -0.6232],\n",
       "        [-0.6352, -0.0517],\n",
       "        [ 0.0540,  0.0633],\n",
       "        [ 0.2827, -0.3141],\n",
       "        [ 0.1157,  0.1450],\n",
       "        [-0.3574, -0.1706],\n",
       "        [-0.4378, -0.4050],\n",
       "        [ 0.0473,  0.1232]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.LSTM(5, 2)\n",
    "dp_module = mWeightDropout(module, 0.4)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -1.0386],\n",
       "        [-1.0586, -0.0862],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [ 0.0000, -0.5236],\n",
       "        [ 0.0000,  0.2416],\n",
       "        [-0.5956, -0.2843],\n",
       "        [-0.0000, -0.0000],\n",
       "        [ 0.0000,  0.2053]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.randn(4,20,5)\n",
    "h = (torch.zeros(1,20,2), torch.zeros(1,20,2))\n",
    "x,h = dp_module(tst_input,h)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dropout is applied to the weights during the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have everything ready to implement the entire AWSD LSTM model, the following code might look really complicated at first but it is in fact pretty much the same as our fullLSTM except we use the different kinds of dropout disscussed above. It also takes care of the word embeddings whereas our fullLSTM assumed it was already done so we need to take care of that. Another difference is that the last layer outputs a different size tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mAWD_LSTM(nn.Module):\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        \"\"\"Returns an iterator over module parameters.\n",
    "\n",
    "        This is typically passed to an optimizer.\n",
    "\n",
    "        Args:\n",
    "            vocab_sz (int): number of words in the vocab\n",
    "            emb_sz (int): size of the word embedding vector\n",
    "            n_hid (int): size of the hidden vector \n",
    "            n_layers (int): number of layers in the LSTM\n",
    "            pad_token (int): id of the pad_idx for the embedding matrix\n",
    "            hidden_p (float): dropout probability for variational dropout on hidden activations\n",
    "            input_p (float): dropout probability for variational dropout on input activations\n",
    "            embed_p (float):dropout probability for embedding dropout \n",
    "            weight_p (float):dropout probability for weight dropout\n",
    "\n",
    "        \"\"\"\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = mEmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([mWeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = VDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([VDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        print(raw_output.shape)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a decoder which takes the output of our AWD_LSTM and transform it into the prediction of the wrord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = VDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine both of them using a sequential module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mSequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mget_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n",
    "                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = mAWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, mLinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code from the last couple of cells is actually already implemented in the fastai library with minor changes with the same functions/class names without the m at the beggining. For example here is the model generated from our implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): mAWD_LSTM(\n",
       "    (emb): Embedding(400, 20, padding_idx=399)\n",
       "    (emb_dp): mEmbeddingDropout(\n",
       "      (emb): Embedding(400, 20, padding_idx=399)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): mWeightDropout(\n",
       "        (module): LSTM(20, 100, batch_first=True)\n",
       "      )\n",
       "      (1): mWeightDropout(\n",
       "        (module): LSTM(100, 100, batch_first=True)\n",
       "      )\n",
       "      (2): mWeightDropout(\n",
       "        (module): LSTM(100, 20, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): VDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): VDropout()\n",
       "      (1): VDropout()\n",
       "      (2): VDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): mLinearDecoder(\n",
       "    (output_dp): VDropout()\n",
       "    (decoder): Linear(in_features=20, out_features=400, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mget_language_model(vocab_sz=400, emb_sz=20, n_hid=100, n_layers=3, pad_token=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate the dame using fastai's get_language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(400, 20, padding_idx=399)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(400, 20, padding_idx=399)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(20, 100, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(100, 100, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(100, 20, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=20, out_features=400, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = {'emb_sz':20, 'n_hid':100, 'n_layers':3, 'pad_token':-1, 'output_p':0.4, 'hidden_p':0.2, 'input_p':0.6, \n",
    "                       'embed_p':0.1, 'weight_p':0.5, 'tie_weights':True, 'out_bias':True}\n",
    "lm = get_language_model(AWD_LSTM, 400, configs)\n",
    "lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, they are both the same with only module names changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_lm(vocab) :    \n",
    "    lm = get_language_model(AWD_LSTM, len(vocab))\n",
    "    model_path = untar_data('https://s3.amazonaws.com/fast-ai-modelzoo/wt103-1', data=False)\n",
    "    fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "    old_itos = pickle.load(open(fnames[1], 'rb'))\n",
    "    old_stoi = {v:k for k,v in enumerate(old_itos)}\n",
    "    wgts = torch.load(fnames[0], map_location=lambda storage, loc: storage)\n",
    "    wgts = convert_weights(wgts, old_stoi, vocab.itos)\n",
    "    lm.load_state_dict(wgts)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the basic implementation of a training loop. It takes a model, an optimizer, the training and validation datasets and the number of epochs and then trains the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Handle batchnorm / dropout\n",
    "        model.train()\n",
    "#         print(model.training)\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "#         print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us refactor a bit and create the classes:\n",
    "- Databunch : which stores the training and the validation dataloaders\n",
    "- Learner : which stores a databunch, the model, the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Databunch() :\n",
    "    def __init__(self, train_dl, valid_dl) :\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "        \n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLearner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our fit fucntion looks like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, learn):\n",
    "    for epoch in range(epochs):\n",
    "        learn.model.train()\n",
    "        for xb,yb in learn.data.train_dl:\n",
    "            loss = learn.loss_func(learn.model(xb), yb)\n",
    "            loss.backward()\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in learn.data.valid_dl:\n",
    "                pred = learn.model(xb)\n",
    "                tot_loss += learn.loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(learn.data.valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import *\n",
    "from fastai import *\n",
    "import torch\n",
    "from fastai.text import *\n",
    "from torchtext.datasets import Multi30k, LanguageModelingDataset\n",
    "from models import *\n",
    "from torchtext.data import *\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, learn, cuda=True, show_info=True, record = True):\n",
    "    \n",
    "    if record:\n",
    "        train_losses = []\n",
    "        val_losses =[]\n",
    "        train_accs = []\n",
    "        valid_accs =[]\n",
    "    if cuda :\n",
    "        learn.model.cuda()\n",
    "    #Start the epoch\n",
    "    for epoch in range(epochs):\n",
    "        #if needed, put model on gpu\n",
    "        learn.model.reset()\n",
    "\n",
    "        \n",
    "        train_size = len(learn.data.train_dl)\n",
    "        valid_size = len(learn.data.valid_dl)\n",
    "        train_loss, valid_loss, train_acc, valid_acc = 0, 0, 0, 0\n",
    "\n",
    "        #puts the model on training mode (activates dropout)\n",
    "        \n",
    "        learn.model.train()\n",
    "        batches = tqdm(learn.data.train_dl, leave=False,\n",
    "                        total=len(learn.data.train_dl), desc=f'Epoch {epoch} training')\n",
    "\n",
    "        #starts sgd for each batches\n",
    "        for batch in batches:\n",
    "\n",
    "            #forward pass\n",
    "            xb = batch.text.t().cuda()\n",
    "            yb = batch.target.t().cuda()\n",
    "            pred = learn.model(xb)[0]\n",
    "            loss = learn.loss_func(pred, yb)\n",
    "\n",
    "            train_loss += loss\n",
    "            train_acc += (torch.argmax(pred, dim=2) == yb).type(torch.FloatTensor).mean() \n",
    "\n",
    "            # compute gradients and updtape parameters\n",
    "            loss.backward()\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        train_loss = train_loss/train_size\n",
    "        train_acc = train_acc/train_size\n",
    "        \n",
    "\n",
    "        # putting the model in eval mode so that dropout is not applied\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batches = tqdm(learn.data.valid_dl, leave=False,\n",
    "                     total=len(learn.data.valid_dl), desc=f'Epoch {epoch} validation')\n",
    "            for batch in batches:\n",
    "                xb = batch.text.t().cuda()\n",
    "                yb = batch.target.t().cuda()\n",
    "                pred = learn.model(xb)[0]\n",
    "                loss = learn.loss_func(pred, yb)\n",
    "\n",
    "                valid_loss += loss\n",
    "                valid_acc += (torch.argmax(pred, dim=2) == yb).type(torch.FloatTensor).mean() \n",
    "                \n",
    "        valid_loss = valid_loss/valid_size\n",
    "        valid_acc = valid_acc/valid_size\n",
    "        \n",
    "        if show_info :\n",
    "            print(\"Epoch {:.0f} training loss : {:.3f}, train accuracy : {:.3f}, validation loss : {:.3f}, valid accuracy : {:.3f}\".format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "        if record :\n",
    "            val_losses.append(valid_loss)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            valid_accs.append(valid_acc)\n",
    "    \n",
    "    if record :\n",
    "        return {'train_loss' : train_losses, 'valid_loss' : val_losses, 'train_acc': train_acc, 'valid_acc' : valid_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradiant clipping\n",
    "- Activation regularization (AR)\n",
    "- Temporal activation regularization (TAR)\n",
    "- discriminative learning rate \n",
    "- slanted triangular learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_awd_lstm(epochs, learn, cuda=True, show_info=True, grad_clip=0.1, alpha=2., beta=1., record=True, one_cycle=True):\n",
    "    \n",
    "    if record:\n",
    "        train_losses = []\n",
    "        val_losses =[]\n",
    "        train_accs = []\n",
    "        valid_accs =[]\n",
    "    #Start the epoch\n",
    "    for epoch in range(epochs):\n",
    "        #if needed, put model on gpu\n",
    "        if cuda :\n",
    "            learn.model.cuda()\n",
    "        train_size = len(learn.data.train_dl)\n",
    "        valid_size = len(learn.data.valid_dl)\n",
    "        train_loss, valid_loss, train_acc, valid_acc = 0, 0, 0, 0\n",
    "\n",
    "        #puts the model on training mode (activates dropout)\n",
    "        \n",
    "        learn.model.train()\n",
    "        \n",
    "            \n",
    "\n",
    "        batches = tqdm(learn.data.train_dl, leave=False,\n",
    "                        total=len(learn.data.train_dl), desc=f'Epoch {epoch} training')\n",
    "\n",
    "        #starts sgd for each batches\n",
    "        for batch in batches:\n",
    "\n",
    "            #forward pass\n",
    "            xb = batch.text.t().cuda()\n",
    "            yb = batch.target.t().cuda()\n",
    "            pred, raw_out, out = learn.model(xb)\n",
    "            loss = learn.loss_func(pred, yb)\n",
    "            #activation regularization \n",
    "            if alpha != 0.:  loss += alpha * out[-1].float().pow(2).mean()\n",
    "            #temporal activation regularization \n",
    "            if beta != 0.:\n",
    "                h = raw_out[-1]\n",
    "                if len(h)>1: loss += beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "            \n",
    "            train_loss += loss\n",
    "            train_acc += (torch.argmax(pred, dim=2) == yb).type(torch.FloatTensor).mean() \n",
    "\n",
    "            # compute gradients and updtape parameters\n",
    "            loss.backward()\n",
    "            if grad_clip:  nn.utils.clip_grad_norm_(learn.model.parameters(), grad_clip)\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        train_loss = train_loss/train_size\n",
    "        train_acc = train_acc/train_size\n",
    "        \n",
    "\n",
    "        # putting the model in eval mode so that dropout is not applied\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batches = tqdm(learn.data.valid_dl, leave=False,\n",
    "                     total=len(learn.data.valid_dl), desc=f'Epoch {epoch} validation')\n",
    "            for batch in batches:\n",
    "                xb = batch.text.t().cuda()\n",
    "                yb = batch.target.t().cuda()\n",
    "                pred = learn.model(xb)[0]\n",
    "                loss = learn.loss_func(pred, yb)\n",
    "\n",
    "                valid_loss += loss\n",
    "                valid_acc += (torch.argmax(pred, dim=2) == yb).type(torch.FloatTensor).mean() \n",
    "                \n",
    "        valid_loss = valid_loss/valid_size\n",
    "        valid_acc = valid_acc/valid_size\n",
    "        \n",
    "        if show_info :\n",
    "            print(\"Epoch {:.0f} training loss : {:.3f}, train accuracy : {:.3f}, validation loss : {:.3f}, valid accuracy : {:.3f}\".format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "        if record :\n",
    "            val_losses.append(valid_loss)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            valid_accs.append(valid_acc)\n",
    "    \n",
    "    if record :\n",
    "        return {'train_loss' : train_losses, 'valid_loss' : val_losses, 'train_acc': train_acc, 'valid_acc' : valid_acc}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_find(learn, start_lr=1e-7, end=10, num_it=100, beta=0.9) :\n",
    "    iteration = 0\n",
    "    best_loss = 0\n",
    "    losses = []\n",
    "    lrs = []\n",
    "    learn.model.cuda()\n",
    "    sm = Smooth\n",
    "    for batch in learn.data.train_dl :\n",
    "        print(iteration)\n",
    "        new_lr = annealing_exp(start_lr, end, iteration/num_it)\n",
    "        for p in learn.opt.param_groups :\n",
    "            p['lr'] = new_lr\n",
    "        \n",
    "        xb = batch.text.t().cuda()\n",
    "        yb = batch.target.t().cuda()\n",
    "        pred = learn.model(xb)[0]\n",
    "        loss = learn.loss_func(pred, yb)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        lrs.append(new_lr)\n",
    "        \n",
    "        \n",
    "        if iteration==0 or loss < best_loss: best_loss = loss\n",
    "        if iteration > num_it or (loss > 4*best_loss or torch.isnan(loss)):\n",
    "            break\n",
    "        iteration+=1\n",
    "\n",
    "    return losses, lrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = get_language_model(AWD_LSTM, len(vocabbb))\n",
    "lm_pretrained = load_pretrained_lm(vocabbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = torch.optim.Adam(lm.parameters(), lr=lr)\n",
    "opt_pretrained = torch.optim.Adam(lm_pretrained.parameters(), lr=lr)\n",
    "\n",
    "data = Databunch(train_it, valid_it)\n",
    "loss_func = CrossEntropyFlat()\n",
    "\n",
    "learner = mLearner(lm, opt, loss_func, data)\n",
    "learner_pretrained = mLearner(lm_pretrained, opt_pretrained, loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 training:  15%|█▍        | 16/109 [00:02<00:14,  6.24it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-a383daf10717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_awd_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner_pretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-81-ba125ed8b449>\u001b[0m in \u001b[0;36mfit_awd_lstm\u001b[0;34m(epochs, learn, cuda, show_info, grad_clip, alpha, beta, record, one_cycle)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# compute gradients and updtape parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zarr/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zarr/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit_awd_lstm(10, learner_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_on_epochs(epochs, learners, names) :\n",
    "    info = {}\n",
    "    for l, n in zip(learners, names) :\n",
    "        info[n] = fit(epochs, l)\n",
    "    return info        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 training:   1%|          | 1/109 [00:00<00:20,  5.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss : 7.402, train accuracy : 0.051, validation loss : 7.626, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 training:   1%|          | 1/109 [00:00<00:20,  5.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss : 7.470, train accuracy : 0.052, validation loss : 7.623, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 training:   1%|          | 1/109 [00:00<00:20,  5.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training loss : 7.464, train accuracy : 0.051, validation loss : 7.635, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 training:   1%|          | 1/109 [00:00<00:20,  5.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training loss : 7.095, train accuracy : 0.052, validation loss : 7.014, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 training:   1%|          | 1/109 [00:00<00:20,  5.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 training loss : 6.961, train accuracy : 0.052, validation loss : 7.111, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 training:   1%|          | 1/109 [00:00<00:20,  5.33it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 training loss : 7.025, train accuracy : 0.051, validation loss : 7.364, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 training:   1%|          | 1/109 [00:00<00:21,  5.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 training loss : 7.015, train accuracy : 0.052, validation loss : 7.372, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 training:   1%|          | 1/109 [00:00<00:19,  5.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 training loss : 6.979, train accuracy : 0.052, validation loss : 7.375, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 training:   1%|          | 1/109 [00:00<00:19,  5.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 training loss : 6.941, train accuracy : 0.052, validation loss : 7.372, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 training:   0%|          | 0/109 [00:00<?, ?it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 training loss : 6.925, train accuracy : 0.052, validation loss : 7.385, valid accuracy : 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 training:   1%|          | 1/109 [00:00<00:20,  5.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss : 6.167, train accuracy : 0.141, validation loss : 5.445, valid accuracy : 0.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 training:   1%|          | 1/109 [00:00<00:19,  5.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss : 5.119, train accuracy : 0.216, validation loss : 5.343, valid accuracy : 0.214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 training:   1%|          | 1/109 [00:00<00:20,  5.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training loss : 4.747, train accuracy : 0.238, validation loss : 5.342, valid accuracy : 0.221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 training:   1%|          | 1/109 [00:00<00:20,  5.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training loss : 4.477, train accuracy : 0.254, validation loss : 5.390, valid accuracy : 0.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 training:   1%|          | 1/109 [00:00<00:20,  5.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 training loss : 4.286, train accuracy : 0.265, validation loss : 5.482, valid accuracy : 0.224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 training:   1%|          | 1/109 [00:00<00:20,  5.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 training loss : 4.135, train accuracy : 0.272, validation loss : 5.527, valid accuracy : 0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 training:   1%|          | 1/109 [00:00<00:19,  5.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 training loss : 3.985, train accuracy : 0.283, validation loss : 5.620, valid accuracy : 0.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 training:   1%|          | 1/109 [00:00<00:19,  5.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 training loss : 3.891, train accuracy : 0.288, validation loss : 5.662, valid accuracy : 0.228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 training:   1%|          | 1/109 [00:00<00:19,  5.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 training loss : 3.781, train accuracy : 0.298, validation loss : 5.709, valid accuracy : 0.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 training loss : 3.687, train accuracy : 0.305, validation loss : 5.721, valid accuracy : 0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "info = compare_on_epochs(10, [learner, learner_pretrained], ['classic', 'pretrained'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/info.pkl', 'rb') as f :\n",
    "    info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classic': {'train_loss': [tensor(7.4020, device='cuda:0', requires_grad=True),\n",
       "   tensor(7.4696, device='cuda:0', requires_grad=True),\n",
       "   tensor(7.4640, device='cuda:0', requires_grad=True),\n",
       "   tensor(7.0947, device='cuda:0', requires_grad=True),\n",
       "   tensor(6.9615, device='cuda:0', requires_grad=True),\n",
       "   tensor(7.0248, device='cuda:0', requires_grad=True),\n",
       "   tensor(7.0152, device='cuda:0', requires_grad=True),\n",
       "   tensor(6.9788, device='cuda:0', requires_grad=True),\n",
       "   tensor(6.9412, device='cuda:0', requires_grad=True),\n",
       "   tensor(6.9246, device='cuda:0', requires_grad=True)],\n",
       "  'valid_loss': [tensor(7.6262, device='cuda:0'),\n",
       "   tensor(7.6228, device='cuda:0'),\n",
       "   tensor(7.6352, device='cuda:0'),\n",
       "   tensor(7.0138, device='cuda:0'),\n",
       "   tensor(7.1111, device='cuda:0'),\n",
       "   tensor(7.3640, device='cuda:0'),\n",
       "   tensor(7.3716, device='cuda:0'),\n",
       "   tensor(7.3751, device='cuda:0'),\n",
       "   tensor(7.3720, device='cuda:0'),\n",
       "   tensor(7.3855, device='cuda:0')],\n",
       "  'train_acc': tensor(0.0525),\n",
       "  'valid_acc': tensor(0.0533)},\n",
       " 'pretrained': {'train_loss': [tensor(6.1668, device='cuda:0', requires_grad=True),\n",
       "   tensor(5.1195, device='cuda:0', requires_grad=True),\n",
       "   tensor(4.7474, device='cuda:0', requires_grad=True),\n",
       "   tensor(4.4774, device='cuda:0', requires_grad=True),\n",
       "   tensor(4.2861, device='cuda:0', requires_grad=True),\n",
       "   tensor(4.1352, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.9849, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.8911, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.7813, device='cuda:0', requires_grad=True),\n",
       "   tensor(3.6872, device='cuda:0', requires_grad=True)],\n",
       "  'valid_loss': [tensor(5.4450, device='cuda:0'),\n",
       "   tensor(5.3431, device='cuda:0'),\n",
       "   tensor(5.3418, device='cuda:0'),\n",
       "   tensor(5.3896, device='cuda:0'),\n",
       "   tensor(5.4820, device='cuda:0'),\n",
       "   tensor(5.5267, device='cuda:0'),\n",
       "   tensor(5.6205, device='cuda:0'),\n",
       "   tensor(5.6617, device='cuda:0'),\n",
       "   tensor(5.7094, device='cuda:0'),\n",
       "   tensor(5.7205, device='cuda:0')],\n",
       "  'train_acc': tensor(0.3046),\n",
       "  'valid_acc': tensor(0.2246)}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "fig = plt.subplot(2, 2)\n",
    "ax[0].plot(range(10), [i.item() for i in info['classic']['valid_loss']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_info(info) :\n",
    "    fig, ax = plt.subplots(2, figsize=(10,10))\n",
    "    metrics = ['train_loss', 'valid_loss']\n",
    "    for i, m in enumerate(metrics) :\n",
    "        for learner in info :\n",
    "            ax[i].plot(range(10), [t.item() for t in info[learner][m]], label=learner)\n",
    "        ax[i].set_title(m)\n",
    "        ax[i].legend()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_info(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
