{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import *\n",
    "from fastai import *\n",
    "import torch\n",
    "from fastai.text import *\n",
    "from torchtext.datasets import Multi30k, LanguageModelingDataset\n",
    "from models import *\n",
    "from torchtext.data import *\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import os\n",
    "import csv\n",
    "from itertools import chain\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataSet(data.Dataset):\n",
    "    \"\"\"Defines a dataset for language modeling.\"\"\"\n",
    "\n",
    "    def __init__(self, path, text_field, newline_eos=True,\n",
    "                 encoding='utf-8', **kwargs):\n",
    "        \"\"\"Create a LanguageModelingDataset given a path and a field.\n",
    "\n",
    "        Arguments:\n",
    "            path: Path to the data file.\n",
    "            text_field: The field that will be used for text data.\n",
    "            newline_eos: Whether to add an <eos> token for every newline in the\n",
    "                data file. Default: True.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        fields = [('text', text_field)]\n",
    "        text = []\n",
    "        with open(path, encoding=encoding) as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            count=0\n",
    "            for row in reader:\n",
    "                count+=1\n",
    "                if count ==1 :\n",
    "                    continue \n",
    "                text += text_field.preprocess(row[1])\n",
    "                if newline_eos:\n",
    "                    text.append(u'<eos>')\n",
    "                \n",
    "\n",
    "        examples = [data.Example.fromlist([text], fields)]\n",
    "        super(LMDataSet, self).__init__(\n",
    "            examples, fields, **kwargs)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_sample(self, ratio=0.2) :\n",
    "        text = self.examples[0].text\n",
    "        fs =len(text)\n",
    "        text_sample = text[:int(fs*ratio)]\n",
    "        ex_sample = Example.fromlist([text_sample], list(self.fields.items()))\n",
    "        return Dataset([ex_sample], list(self.fields.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(lm_dataset, ratio=0.8) :\n",
    "        text = lm_dataset.examples[0].text\n",
    "        fs =len(text)\n",
    "        \n",
    "        train = text[:int(fs*ratio)]\n",
    "        valid = text[int(fs*ratio):]\n",
    "        ex_train = Example.fromlist([train], list(lm_dataset.fields.items()))\n",
    "        ex_valid = Example.fromlist([valid], list(lm_dataset.fields.items()))\n",
    "        return Dataset([ex_train], list(lm_dataset.fields.items())), Dataset([ex_valid], list(lm_dataset.fields.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ds = LMDataSet('data/train_full.tsv', Field())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ds.fields['text'].build_vocab([lm_ds[0].text], max_size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ds_sample = lm_ds.get_sample(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = split_train_valid(lm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34793900, 8698476, 43492376)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0].text), len(valid[0].text), len(lm_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "bptt = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabbb = train.fields['text'].vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it = BPTTIterator(train, bs, bptt)\n",
    "valid_it = BPTTIterator(valid, bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_lm(vocab) :    \n",
    "    lm = get_language_model(AWD_LSTM, len(vocab))\n",
    "    model_path = untar_data('https://s3.amazonaws.com/fast-ai-modelzoo/wt103-1', data=False)\n",
    "    fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "    old_itos = pickle.load(open(fnames[1], 'rb'))\n",
    "    old_stoi = {v:k for k,v in enumerate(old_itos)}\n",
    "    wgts = torch.load(fnames[0], map_location=lambda storage, loc: storage)\n",
    "    wgts = convert_weights(wgts, old_stoi, vocab.itos)\n",
    "    lm.load_state_dict(wgts)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Databunch() :\n",
    "    def __init__(self, train_dl, valid_dl) :\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "        \n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLearner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data\n",
    "    \n",
    "    def freeze_to(self, n) :\n",
    "        assert(n < len(self.opt.param_groups))\n",
    "        for g in self.opt.param_groups[:n]:\n",
    "            for l in g['params']:\n",
    "                l.requires_grad=False\n",
    "        for g in self.opt.param_groups[n:]: \n",
    "            for l in g['params']:\n",
    "                l.requires_grad=True\n",
    "    def unfreeze(self) :\n",
    "        self.freeze_to(0)\n",
    "    def freeze(self) :\n",
    "        for g in self.opt.param_groups:\n",
    "            for l in g['params']:\n",
    "                l.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_param_groups(model) :\n",
    "    parameters = [] \n",
    "    for i in range(3) :\n",
    "        layer = f'{i}'\n",
    "        parameters.append({'params' :lm_pretrained._modules['0']._modules['rnns']._modules[layer].parameters()})\n",
    "    modules = chain(lm_pretrained._modules['1'].parameters(), lm_pretrained._modules['0']._modules['encoder'].parameters())\n",
    "    parameters.append({'params': modules})\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_awd_lstm(epochs, learn, cuda=True, show_info=True, grad_clip=0.1, alpha=2., beta=1., record=True, one_cycle=True, \n",
    "                 cut_frac = 0.1, n_max = 0.01, ratio=32, discr=True, discr_rate=2.6):\n",
    "    \n",
    "    #number of batches in one epoch for validation and training data\n",
    "    train_size = len(learn.data.train_dl)\n",
    "    valid_size = len(learn.data.valid_dl)\n",
    "    \n",
    "    # total iterations and cut used for slanted_triangular learning rates (T and cut from paper)\n",
    "    total_iterations = epochs*train_size\n",
    "    cut = int(total_iterations*cut_frac)\n",
    "\n",
    "    \n",
    "    if record:\n",
    "        lrs = []\n",
    "        train_losses = []\n",
    "        val_losses =[]\n",
    "        train_accs = []\n",
    "        valid_accs =[]\n",
    "    \n",
    "    #puts model on gpu\n",
    "    if cuda :\n",
    "        learn.model.cuda()\n",
    "    \n",
    "    #Start the epoch\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #loss and accuracy \n",
    "        train_loss, valid_loss, train_acc, valid_acc = 0, 0, 0, 0\n",
    "\n",
    "        #puts the model on training mode (activates dropout)\n",
    "        learn.model.train()\n",
    "        \n",
    "        #iterator over all batches in training\n",
    "        batches = tqdm_notebook(learn.data.train_dl, leave=False,\n",
    "                        total=len(learn.data.train_dl), desc=f'Epoch {epoch} training')\n",
    "        \n",
    "        #batch number counter\n",
    "        batch_num = 0\n",
    "       \n",
    "        #starts sgd for each batches\n",
    "        for batch in batches:\n",
    "            \n",
    "            #Slanted_triangular learning rates\n",
    "            if one_cycle :\n",
    "                iteration = (epoch * train_size) + batch_num\n",
    "                assert(total_iterations >= iteration)\n",
    "\n",
    "                if iteration < cut :\n",
    "                    p=iteration/cut\n",
    "                else :\n",
    "                    p = 1-( (iteration-cut) / (cut*(1/cut_frac-1) ))\n",
    "                    p = max(p, 0)\n",
    "                new_lr = n_max*( (1 + p*(ratio-1)) / ratio )\n",
    "                \n",
    "                for p in learn.opt.param_groups :\n",
    "                    p['lr'] = new_lr\n",
    "                lrs.append(new_lr)\n",
    "            batch_num+=1\n",
    "\n",
    "            #disdcriminative learning rate \n",
    "            if discr :\n",
    "                for i in range(3) :#all  3 layers starting from last one \n",
    "                    learn.opt.param_groups[-(i+2)]['lr'] = learn.opt.param_groups[-(i+2)]['lr']/ (discr_rate)**i\n",
    "\n",
    "            #forward pass\n",
    "            xb = batch.text.t().cuda()\n",
    "            yb = batch.target.t().cuda()\n",
    "            pred, raw_out, out = learn.model(xb)\n",
    "            loss = learn.loss_func(pred, yb)\n",
    "            \n",
    "            #activation regularization \n",
    "            if alpha != 0.:  loss += alpha * out[-1].float().pow(2).mean()\n",
    "            \n",
    "            #temporal activation regularization \n",
    "            if beta != 0.:\n",
    "                h = raw_out[-1]\n",
    "                if len(h)>1: loss += beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "            \n",
    "            train_loss += loss\n",
    "            train_acc += (torch.argmax(pred, dim=2) == yb).type(torch.FloatTensor).mean() \n",
    "\n",
    "            # compute gradients and updtape parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            #gradient clipping\n",
    "            if grad_clip:  nn.utils.clip_grad_norm_(learn.model.parameters(), grad_clip)\n",
    "            \n",
    "            #optimizationm step\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        train_loss = train_loss/train_size\n",
    "        train_acc = train_acc/train_size\n",
    "        \n",
    "\n",
    "        # putting the model in eval mode so that dropout is not applied\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batches = tqdm_notebook(learn.data.valid_dl, leave=False,\n",
    "                     total=len(learn.data.valid_dl), desc=f'Epoch {epoch} validation')\n",
    "            for batch in batches:\n",
    "                xb = batch.text.t().cuda()\n",
    "                yb = batch.target.t().cuda()\n",
    "                pred = learn.model(xb)[0]\n",
    "                loss = learn.loss_func(pred, yb)\n",
    "\n",
    "                valid_loss += loss\n",
    "                valid_acc += (torch.argmax(pred, dim=2) == yb).type(torch.FloatTensor).mean() \n",
    "                \n",
    "        valid_loss = valid_loss/valid_size\n",
    "        valid_acc = valid_acc/valid_size\n",
    "        \n",
    "        if show_info :\n",
    "            print(\"Epoch {:.0f} training loss : {:.3f}, train accuracy : {:.3f}, validation loss : {:.3f}, valid accuracy : {:.3f}\".format(epoch, train_loss, train_acc, valid_loss, valid_acc))\n",
    "        if record :\n",
    "            val_losses.append(valid_loss)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            valid_accs.append(valid_acc)\n",
    "    \n",
    "    if record :\n",
    "        return {'train_loss' : train_losses, 'valid_loss' : val_losses, 'train_acc': train_acc, 'valid_acc' : valid_acc, 'lrs' : lrs}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = get_language_model(AWD_LSTM, len(vocabbb))\n",
    "lm_pretrained = load_pretrained_lm(vocabbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = torch.optim.Adam(get_model_param_groups(lm_pretrained), lr=lr)\n",
    "opt_pretrained = torch.optim.Adam(get_model_param_groups(lm_pretrained), lr=lr)\n",
    "\n",
    "data = Databunch(train_it, valid_it)\n",
    "loss_func = CrossEntropyFlat()\n",
    "\n",
    "learner = mLearner(lm, opt, loss_func, data)\n",
    "learner_pretrained = mLearner(lm_pretrained, opt_pretrained, loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_pretrained.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total iterations : 109, cut : 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0 training', max=109, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0 validation', max=28, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss : 5.930, train accuracy : 0.171, validation loss : 5.259, valid accuracy : 0.220\n"
     ]
    }
   ],
   "source": [
    "info = fit_awd_lstm(1, learner_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total iterations : 327, cut : 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0 training', max=109, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0 validation', max=28, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0 training loss : 4.484, train accuracy : 0.275, validation loss : 5.133, valid accuracy : 0.240\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1 training', max=109, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1 validation', max=28, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1 training loss : 4.197, train accuracy : 0.291, validation loss : 5.131, valid accuracy : 0.247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2 training', max=109, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2 validation', max=28, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 2 training loss : 3.807, train accuracy : 0.327, validation loss : 5.172, valid accuracy : 0.251\n"
     ]
    }
   ],
   "source": [
    "info = fit_awd_lstm(3, learner_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(info) :\n",
    "    if not isinstance(info, dict) :\n",
    "        info = {'' : info}\n",
    "    fig, ax = plt.subplots(5, figsize=(10,10))\n",
    "    metrcis = ['train_loss' , 'valid_loss', 'train_acc' , 'valid_acc' , 'lrs' ]\n",
    "    for model in info :\n",
    "        for i, met in enumerate(metrcis) :\n",
    "            ax[i].plot(range(len(info[model][met])), info[model][met], label=model)\n",
    "            ax[i].set_xlabel('iterations')\n",
    "            ax[i].set_ylabel(met)\n",
    "            ax[i].legend()\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
