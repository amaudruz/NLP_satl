{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai import *\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD LSTM Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will go through the full implementation of the AWD LSTM neural net architecture and training used as language model in the [ULMFIT](https://arxiv.org/pdf/1801.06146.pdf) paper. Most of the upcoming code is heavily based on the [fastai](https://docs.fast.ai) library and its deep learning course, which has already a full implementation of the ulmfit approach for NLP. However considering the complexity of the fastai code and its simplicity to use we figured it would helpful for readers to get a full bottom up implementation using pytorch as a baseline. Still we expect that if you read this notebook, you have a good knowledge and understanding of RNNs, language modeling (see paper) and pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of the AWD LSTM architecture is of course the LSTM neural net. It is an improvement to the standart RNN way of dealing with sequential data (such as text). LSTM deals with the [vanishing/exploding gradient problem](https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb) that come up in simple RNNs using cell connection gates. For further intuition on 'why' most the upcoming implentations I recommend colah's blog post : [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM cell and equations](images/lstm.jpg)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM archutecture is composed of a repeated cell which is shown in the image above. Its inputs are :\n",
    "- **xt** which in our case is the embedding vector of the nth word of a batch of sentences\n",
    "- **ht-1** the output of the last cell just like in RNNs.\n",
    "- **ct-1** again output form last cell which is called the *cell state* used to prevent long-term dependencies problem.\n",
    "\n",
    "The $\\sigma$ reprenstents the [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function applied element-wise to its input. Both x and + connections are elemnt-wise multiplication and addition respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement it using the pytorch nn.Module class. We use a two big matrix multiplication to compute x*U and and h*U instead of 4 for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, x_s, h_s):\n",
    "        super().__init__()\n",
    "        self.h_s = h_s\n",
    "        self.x_s = x_s\n",
    "        self.U = nn.Linear(x_s,4*h_s)\n",
    "        self.W = nn.Linear(h_s,4*h_s)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        #inputs from last cell\n",
    "        h,c = state\n",
    "        \n",
    "        #computing itermedtiate gates\n",
    "        gates = (self.U(input) + self.W(h)).chunk(4, 1)\n",
    "        i_t,f_t,o_t = map(torch.sigmoid, gates[:3])\n",
    "        c_t = gates[3].tanh()\n",
    "        c = (f_t*c) + (i_t*c_t)\n",
    "        h = o_t * c.tanh()\n",
    "        \n",
    "        #outputting the usualt h output and the state to give to next cell if needed\n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next building block of the LSTM is the LSTM layer wich consisit of appling the LSTM cell to each sequential input in a recurrent manner with each time forwarding its state to the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM layer](images/LSTM3.png)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, x_s, h_s):\n",
    "        super().__init__()\n",
    "        self.lstm_cell = LSTMCell(x_s, h_s)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        # divide the input in the sequence dimension to get x_0, x_1, x_2, ...\n",
    "        inputs = input.unbind(1)\n",
    "        \n",
    "        #prepare to store the output of each cell\n",
    "        outputs = []\n",
    "        \n",
    "        #applying the cell recursively \n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.lstm_cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        \n",
    "        #return the stacked outputs\n",
    "        return torch.stack(outputs, dim=1), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the state for the first cell we simply use tensors with only zeroes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before having fully implemented pytorch's LSTM module is stacking multiple LSTM layers one above each other as shown in the following diagram :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacked RNN](images/RNN_Stacking.png)\n",
    "(picture from : https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/recurrent_neural_networks.html?q= )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='pink'>pink rectangles</font>  : The sequential inputs  \n",
    "- <font color='green'>green rectangles</font>  : An LSTM cell, each line is a layer so the cells on the same line are the same\n",
    "- <font color='blue'>blue rectangles</font> : The sequential outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous implementations, we created the initial state **h0** and **c0** outside of the model at the same time as we gave the input. This means that we could create h0 and c0 with the right sizes by simply comparing it to the input sizes. This time around, we will create the initial state to give to all the layers inside the model itself. As the dimensions of the state depend on the batch size of the input given, we need to create the initial states at the start of the forward pass when we are given the input (and thus we know the batch size) with the help of the **reset()** method. We also want to keep the last states from last batch if the batch size did not change.\n",
    "\n",
    "We aso have to be careful of the sives of the hidden layers inputs. For example, on the image above, the first layer takes as input the initial sequence and outputs a hidden sequence whereas the second and third layers take as input a hidden sequence and outputs a hidden sequence. Because of that we must have a different sizes for the first layer and the other layers. And of course the same goes for the initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullLSTM(nn.Module):\n",
    "    def __init__(self, x_s, h_s, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm_layers = nn.ModuleList([LSTMLayer(x_s if i==0 else h_s, h_s ) for i in range(n_layers)])\n",
    "        self.bs = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        #get the batch size from the first dimension of the input \n",
    "        bs, sl, _ = input.size()\n",
    "        if self.bs != bs :\n",
    "            self.bs = bs\n",
    "            self.reset()\n",
    "       \n",
    "        # now we have the initial states and we can go through all the layers recursively\n",
    "        for j in range(self.n_layers) :\n",
    "            layer = self.lstm_layers[j]\n",
    "            input, self.hidden[j] = layer(input, self.hidden[j])\n",
    "                \n",
    "        \n",
    "        #return the outputs\n",
    "        return input\n",
    "    \n",
    "    def reset(self) :\n",
    "        st = next(self.parameters()).new(self.bs, h_s).zero_()\n",
    "        self.hidden = [(st, st) for l in range(self.n_layers)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is pretty much the same as pytorch's nn.LSTM. The difference is that pytorch uses CuDNN to make the computations faster. We will now use pytorch's implementation instead of ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization : Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual regularization techniques used in feed-forward and convolutional neural nets such as dropout and batchnorm do not work well in RNNs. The AWD LSTM uses extensions of those to regularize its model. Correspondigns ections of the [paper](https://arxiv.org/pdf/1708.02182.pdf) will be provided for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational dropout \n",
    "Section 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea in variational dropout is to use the **same** drop out mask to a squential input over the sequence dimension. In essence, if you have an input x with shape *(bs, seq_len, x_s)*, the dropout mask will be of shape *(bs, 1, x_s)* and will be applied to each slice of sequence.\n",
    "This dropout will be used on each output/input of the LSTM layers. Additionally we divide every activations that have not been set to 0 by the mask by 1-p (p: probability of dropout) to keep the average. We use [broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html) to be efficient in the element-wise computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VDropout(nn.Module) :\n",
    "    def __init__(self, p=0.5) :\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x) :\n",
    "        #The dropout should only be used during training and not eval \n",
    "        if not self.training or self.p == 0.: return x\n",
    "        #the mask\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        #element-wise multiplication with broadcasting\n",
    "        return x*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-7.8544e-01, -5.1472e-01, -4.6579e-01,  2.8433e-01, -7.7436e-01,\n",
       "            9.8517e-02, -4.6922e-01],\n",
       "          [ 1.4382e-01, -3.4394e-03,  6.0392e-01, -1.7472e+00, -1.6012e+00,\n",
       "           -1.6544e-01, -1.7867e-02],\n",
       "          [ 2.2054e+00, -1.3237e+00, -1.1873e+00, -1.6786e+00,  9.5719e-01,\n",
       "            3.6004e-01,  1.8659e-01]],\n",
       " \n",
       "         [[-5.2384e-01, -1.1109e+00,  1.3991e+00,  1.1631e+00,  1.8918e+00,\n",
       "           -8.3174e-01,  3.0847e+00],\n",
       "          [ 5.4055e-01, -2.6438e-01,  5.7624e-02,  7.2762e-01,  2.3176e-02,\n",
       "           -5.0091e-01, -1.6765e+00],\n",
       "          [ 1.9560e-01, -1.7617e-01, -1.3988e+00,  3.2684e-01, -4.3012e-01,\n",
       "            1.0110e+00, -9.9373e-01]],\n",
       " \n",
       "         [[-2.3163e-01, -2.6485e-01,  1.5885e+00, -4.9035e-01,  3.1360e-01,\n",
       "            4.5559e-01,  7.9295e-02],\n",
       "          [ 1.1516e+00,  4.5962e-01, -9.3294e-01, -1.1534e+00, -8.1836e-01,\n",
       "           -1.7186e+00,  2.0256e-03],\n",
       "          [ 4.1413e-01, -8.5411e-02, -6.8403e-01,  6.1888e-01, -5.0981e-02,\n",
       "           -5.7498e-01, -3.1320e+00]]]),\n",
       " tensor([[[-1.1221e+00, -0.0000e+00, -6.6541e-01,  4.0618e-01, -0.0000e+00,\n",
       "            1.4074e-01, -6.7032e-01],\n",
       "          [ 2.0546e-01, -0.0000e+00,  8.6274e-01, -2.4960e+00, -0.0000e+00,\n",
       "           -2.3635e-01, -2.5525e-02],\n",
       "          [ 3.1505e+00, -0.0000e+00, -1.6962e+00, -2.3980e+00,  0.0000e+00,\n",
       "            5.1434e-01,  2.6656e-01]],\n",
       " \n",
       "         [[-7.4835e-01, -0.0000e+00,  0.0000e+00,  1.6615e+00,  2.7026e+00,\n",
       "           -1.1882e+00,  0.0000e+00],\n",
       "          [ 7.7222e-01, -0.0000e+00,  0.0000e+00,  1.0395e+00,  3.3108e-02,\n",
       "           -7.1559e-01, -0.0000e+00],\n",
       "          [ 2.7944e-01, -0.0000e+00, -0.0000e+00,  4.6692e-01, -6.1445e-01,\n",
       "            1.4442e+00, -0.0000e+00]],\n",
       " \n",
       "         [[-3.3090e-01, -0.0000e+00,  2.2693e+00, -7.0050e-01,  4.4800e-01,\n",
       "            6.5084e-01,  1.1328e-01],\n",
       "          [ 1.6452e+00,  0.0000e+00, -1.3328e+00, -1.6477e+00, -1.1691e+00,\n",
       "           -2.4551e+00,  2.8938e-03],\n",
       "          [ 5.9161e-01, -0.0000e+00, -9.7718e-01,  8.8411e-01, -7.2830e-02,\n",
       "           -8.2140e-01, -4.4743e+00]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = VDropout(0.3)\n",
    "tst_input = torch.randn(3,3,7)\n",
    "tst_input, m(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the dropped is consistent in the second dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding dropout \n",
    "Section 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embedding dropout we simply nulifiy entire rows of the word embedding matrix with probability p. Again broadcastiong is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mEmbeddingDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [-4.1411,  3.0208,  1.1749,  4.6216,  1.9505,  2.4574,  1.1441],\n",
       "        [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [-1.3713,  0.0883, -1.1435,  0.8223, -0.0501,  0.9796,  2.3210],\n",
       "        [-0.6008, -0.0644, -1.6057, -2.6271, -5.8882, -0.1454,  1.7396],\n",
       "        [-0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [ 1.1096,  2.9371, -0.5824,  1.0254, -0.6754, -3.3056,  0.0240]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = mEmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0,100,(8,))\n",
    "enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that entire rows have been dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight-dropout\n",
    "Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight dropout is a dropout applied to the weights inside the LSTM cells : U and W.\n",
    "\n",
    "In order to keep the speed of the LSTM layer, we simply replace the weight matrix of the LSTM by a masked version and keep the non-masked version. We can then simply apply the LSTM layer and it will use its new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the parameter in the nn.LSTM module containing the weights \n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class mWeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            #\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5450, -0.5859],\n",
       "        [-0.5636, -0.0322],\n",
       "        [-0.2775, -0.4427],\n",
       "        [ 0.6224,  0.2344],\n",
       "        [ 0.3598, -0.3882],\n",
       "        [ 0.3013,  0.6286],\n",
       "        [-0.6047, -0.1457],\n",
       "        [-0.5148,  0.3647]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.LSTM(5, 2)\n",
    "dp_module = mWeightDropout(module, 0.4)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9083, -0.9765],\n",
       "        [-0.0000, -0.0537],\n",
       "        [-0.4625, -0.7379],\n",
       "        [ 1.0373,  0.3907],\n",
       "        [ 0.5997, -0.0000],\n",
       "        [ 0.5022,  1.0476],\n",
       "        [-0.0000, -0.0000],\n",
       "        [-0.8580,  0.6078]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.randn(4,20,5)\n",
    "h = (torch.zeros(1,20,2), torch.zeros(1,20,2))\n",
    "x,h = dp_module(tst_input,h)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dropout is applied to the weights during the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have everything ready to implement the entire AWSD LSTM model, the following code might look really complicated at first but it is in fact pretty much the same as our fullLSTM except we use the different kinds of dropout disscussed above. It also takes care of the word embeddings whereas our fullLSTM assumed it was already done so we need to take care of that. Another difference is that the last layer outputs a different size tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mAWD_LSTM(nn.Module):\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        \"\"\"Returns an iterator over module parameters.\n",
    "\n",
    "        This is typically passed to an optimizer.\n",
    "\n",
    "        Args:\n",
    "            vocab_sz (int): number of words in the vocab\n",
    "            emb_sz (int): size of the word embedding vector\n",
    "            n_hid (int): size of the hidden vector \n",
    "            n_layers (int): number of layers in the LSTM\n",
    "            pad_token (int): id of the pad_idx for the embedding matrix\n",
    "            hidden_p (float): dropout probability for variational dropout on hidden activations\n",
    "            input_p (float): dropout probability for variational dropout on input activations\n",
    "            embed_p (float):dropout probability for embedding dropout \n",
    "            weight_p (float):dropout probability for weight dropout\n",
    "\n",
    "        \"\"\"\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = mEmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([mWeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = VDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([VDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        print(raw_output.shape)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a decoder which takes the output of our AWD_LSTM and transform it into the prediction of the wrord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = VDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine both of them using a sequential module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mSequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mget_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n",
    "                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = mAWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, mLinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code from the last couple of cells is actually already implemented in the fastai library with minor changes with the same functions/class names without the m at the beggining. So from now on I will use fastai's implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will look at the training part of he AWD LSTM language model. The traning loop that we need need to implement actually gets quite complicated if we want to write it without any code refactoring. To this end, I will implement the idea of the [callback](https://docs.fast.ai/callbacks.html) from the fastai library. We will gradually go through it's implementation so that readers do not get lost in the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the basic implementation of a training loop. It takes a model, an optimizer, the training and validation datasets and the number of epochs and then trains the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Handle batchnorm / dropout\n",
    "        model.train()\n",
    "#         print(model.training)\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "#         print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us refactor a bit and create the classes:\n",
    "- Databunch : which stores the training and the validation dataloaders\n",
    "- Learner : which stores a databunch, the model, the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Databunch() :\n",
    "    def __init__(self, train_dl, valid_dl) :\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = valid_dl\n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "        \n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLearner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our fit fucntion looks like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, learn):\n",
    "    for epoch in range(epochs):\n",
    "        learn.model.train()\n",
    "        for xb,yb in learn.data.train_dl:\n",
    "            loss = learn.loss_func(learn.model(xb), yb)\n",
    "            loss.backward()\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in learn.data.valid_dl:\n",
    "                pred = learn.model(xb)\n",
    "                tot_loss += learn.loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(learn.data.valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to modifiy the training loop without having to write another one eah time, we will use callbacks. A callback is is an object that will be called at different stages of the taining loop. This allows a to modify the training in a multitude of ways (hyper paramenter scheduling/recording, regularization techniques etc) and as mentionned, creating another training loop comes down to giving another set of callbacks. For more information refer to  [Fast.ai — An infinitely customizable training loop with Sylvain Gugger](https://www.youtube.com/watch?v=roc-dOSeehM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![callbcaks](images/callbacks.png)\n",
    "(picture from [Fast.ai — An infinitely customizable training loop with Sylvain Gugger\n",
    "](https://www.youtube.com/watch?v=roc-dOSeehM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go through a simplified version of fastai's callback implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mCallback():\n",
    "    def begin_fit(self, learn):\n",
    "        self.learn = learn\n",
    "        self.in_train=True\n",
    "        return True\n",
    "    def after_fit(self): return True\n",
    "    def begin_epoch(self, epoch):\n",
    "        self.epoch=epoch\n",
    "        return True\n",
    "    def begin_validate(self): \n",
    "        print('james')\n",
    "        self.in_train= False\n",
    "        return True\n",
    "    def after_epoch(self): return True\n",
    "    def begin_batch(self, xb, yb):\n",
    "        self.xb,self.yb = xb,yb\n",
    "        return True\n",
    "    def after_loss(self, loss):\n",
    "        self.loss = loss\n",
    "        return True\n",
    "    def after_backward(self): return True\n",
    "    def after_step(self): return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mCallbackHandler():\n",
    "    \"\"\"\n",
    "    Handles a list of callbacks\n",
    "    \"\"\"\n",
    "    def __init__(self,cbs=None):\n",
    "        self.cbs = cbs if cbs else []\n",
    "\n",
    "    def begin_fit(self, learn):\n",
    "        self.learn,self.in_train = learn,True\n",
    "        learn.stop = False\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_fit(learn)\n",
    "        return res\n",
    "\n",
    "    def after_fit(self):\n",
    "        res = not self.in_train\n",
    "        for cb in self.cbs: res = res and cb.after_fit()\n",
    "        return res\n",
    "    \n",
    "    def begin_epoch(self, epoch):\n",
    "        self.learn.model.train()\n",
    "        self.in_train=True\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_epoch(epoch)\n",
    "        return res\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.learn.model.eval()\n",
    "        self.in_train=False\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_validate()\n",
    "        return res\n",
    "\n",
    "    def after_epoch(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.after_epoch()\n",
    "        return res\n",
    "    \n",
    "    def begin_batch(self, xb, yb):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.begin_batch(xb, yb)\n",
    "        return res\n",
    "\n",
    "    def after_loss(self, loss):\n",
    "        res = self.in_train\n",
    "        for cb in self.cbs: res = res and cb.after_loss(loss)\n",
    "        return res\n",
    "\n",
    "    def after_backward(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.after_backward()\n",
    "        return res\n",
    "\n",
    "    def after_step(self):\n",
    "        res = True\n",
    "        for cb in self.cbs: res = res and cb.after_step()\n",
    "        return res\n",
    "    \n",
    "    def do_stop(self):\n",
    "        try:     return self.learn.stop\n",
    "        finally: self.learn.stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of our fit function using callbacks and a bit of refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mone_batch(xb, yb, cb):\n",
    "    if not cb.begin_batch(xb,yb): return\n",
    "    loss = cb.learn.loss_func(cb.learn.model(xb), yb)\n",
    "    if not cb.after_loss(loss): return\n",
    "    loss.backward()\n",
    "    if cb.after_backward(): cb.learn.opt.step()\n",
    "    if cb.after_step(): cb.learn.opt.zero_grad()\n",
    "\n",
    "def mall_batches(dl, cb):\n",
    "    for xb,yb in dl:\n",
    "        mone_batch(xb, yb, cb)\n",
    "        if cb.do_stop(): return\n",
    "\n",
    "def mfit(epochs, learn, cb):\n",
    "    if not cb.begin_fit(learn): return\n",
    "    for epoch in range(epochs):\n",
    "        if not cb.begin_epoch(epoch): continue\n",
    "        mall_batches(learn.data.train_dl, cb)\n",
    "        \n",
    "        if cb.begin_validate():\n",
    "            with torch.no_grad(): mall_batches(learn.data.valid_dl, cb)\n",
    "        if cb.do_stop() or not cb.after_epoch(): break\n",
    "    cb.after_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this might be a lot of this to digest, here is a simple example of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(range(100)).view(-1, 1)\n",
    "y =  2*x  + torch.randn(100, 1) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x.view(-1), y.view(-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to learn y from x. Let's create the train and valid dataloaders and the databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = TensorDataset(x, y)\n",
    "full_dl = DataLoader(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutaion = np.random.permutation(100)\n",
    "train_idx = permutaion[:80]\n",
    "valid_idx = permutaion[80:]\n",
    "train_ds = TensorDataset(x[train_idx], y[train_idx])\n",
    "valid_ds = TensorDataset(x[valid_idx], y[valid_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = DataLoader(train_ds, batch_size=10),  DataLoader(valid_ds, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Databunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need create our learner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(1, 1)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "loss_func = mse\n",
    "learn = mLearner(model, opt, loss_func, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all we need to train, let's create a callback that will print the training and validation loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossPrinter(mCallback) :\n",
    "    def begin_epoch(self, epoch) :\n",
    "        self.epoch = epoch\n",
    "        self.training_loss = 0\n",
    "        self.training_count = 0\n",
    "        self.validation_loss = 0\n",
    "        self.validation_count = 0\n",
    "        return True\n",
    "    \n",
    "    def after_loss(self, loss) :\n",
    "        self.loss = loss\n",
    "        if self.in_train :\n",
    "            print(\"train\")\n",
    "            self.training_loss += loss\n",
    "            self.training_count += 1\n",
    "        else :\n",
    "            print(\"val\")\n",
    "            self.validation_loss += loss\n",
    "            self.validation_count += 1\n",
    "        return True\n",
    "    def after_epoch(self) :\n",
    "        tr_loss = self.training_loss/self.training_count\n",
    "        val_loss = self.validation_loss/self.validation_count\n",
    "        print(f\"Epoch {self.epoch} : train loss : {tr_loss}, val loss : {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = mCallbackHandler(cbs=[LossPrinter()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "train\n",
      "james\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-248-9c812b9d1e45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-240-b0c1813c7d33>\u001b[0m in \u001b[0;36mmfit\u001b[1;34m(epochs, learn, cb)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmall_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-239-06f3e657e393>\u001b[0m in \u001b[0;36mafter_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mafter_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-246-40b1e909c163>\u001b[0m in \u001b[0;36mafter_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mafter_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {self.epoch} : train loss : {tr_loss}, val loss : {val_loss}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "mfit(10, learn, cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-24-06f3e657e393>(12)begin_fit()\n",
      "     10         learn.stop = False\n",
      "     11         res = True\n",
      "---> 12         for cb in self.cbs: res = res and cb.begin_fit(learn)\n",
      "     13         return res\n",
      "     14 \n",
      "\n",
      "ipdb> cb\n",
      "<class '__main__.LossPrinter'>\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LossPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.begin_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ac9e66e408>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.scatter(x, yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
